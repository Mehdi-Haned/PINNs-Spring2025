{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f97c4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # use a backend that doesn't need a display\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.integrate import solve_ivp\n",
    "import random\n",
    "from time import perf_counter\n",
    "from contextlib import contextmanager\n",
    "import copy\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "177e978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code taken and modified from tutorial\n",
    "class MLPWithoutOutput(nn.Module):\n",
    "    \"\"\"\n",
    "     A simple feedforward neural network with multiple hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_hidden_layers):\n",
    "        super(MLPWithoutOutput, self).__init__()\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size,dtype=torch.float32), nn.Tanh())\n",
    "        self.hidden_layers = self._make_hidden_layers(n_hidden_layers, hidden_size)\n",
    "\n",
    "    def _make_hidden_layers(self, n_hidden_layers, hidden_size):\n",
    "        layers = []\n",
    "        for _ in range(n_hidden_layers):\n",
    "            layers += [nn.Linear(hidden_size, hidden_size,dtype=torch.float32), nn.Tanh()]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03c78625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPOutput(nn.Module):\n",
    "    \"\"\"\n",
    "     A simple feedforward neural network with multiple hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(MLPOutput, self).__init__()\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size,dtype=torch.float32)\n",
    "        self.parameterSet = nn.Parameter(torch.randn(1, (int)(output_size/2)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a6955fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class colocPoints(Dataset):\n",
    "  def __init__(self, X):\n",
    "    if not torch.is_tensor(X) :\n",
    "      self.X = torch.tensor(X,requires_grad=True).float()\n",
    "      #reshaping to a nx2 matrix since that is what our network expects as input\n",
    "    else:\n",
    "      self.X = X\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      return {\"X\": self.X[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "010a9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SchrodingerEqnResidualLoss(out, outFirstTimeDer, outSecondXDir, xvals, outmodel, evenmask, oddmask):\n",
    "    xvals = xvals.reshape(-1,1)\n",
    "    Re = out[:,evenmask] # shape [N, D]\n",
    "    \n",
    "    dReBydt   = outFirstTimeDer[:,evenmask]\n",
    "    d2ReBydx2 = outSecondXDir[:,evenmask] # shape [N, D]\n",
    "\n",
    "    Im = out[:,oddmask] # shape [N, D]\n",
    "    dImBydt = outFirstTimeDer[:,oddmask]\n",
    "    d2ImBydx2 = outSecondXDir[:,oddmask] # shape [N, D]\n",
    "\n",
    "    V = (outmodel.parameterSet/2)*(xvals**2)\n",
    "    \n",
    "    residual = torch.mean(torch.square(dReBydt + d2ImBydx2/2 - V*Im) + torch.square(dImBydt - d2ReBydx2/2 + V*Re))\n",
    "    return residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "681e6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFullNetworkWithPrecomputing(Reservoir, data, temporalNormalization, spacialNormalization,\n",
    "                                     outmodel, numoutputs, ICs, LeftBoundary, rightBoundary, colocationPoints, \n",
    "                                     ODEWeight, ICWeight, BCWeight, DataWeight, numEpochs, loss_fn, lr, \n",
    "                                     averageLossOverTime, device, verbose = False):\n",
    "    \n",
    "    tEqualsZeroMask = torch.isclose(colocationPoints[:, 0], torch.tensor(0.0, dtype=colocationPoints.dtype)).to(device)\n",
    "\n",
    "    initialODEWeight = ODEWeight\n",
    "    ODEWeightEpoch = 1000\n",
    "\n",
    "    #initialising optimise\n",
    "    FullPINNOptimizer = optim.Adam(list(Reservoir.parameters()) + list(outmodel.parameters()), lr=lr)\n",
    "\n",
    "    scheduler = lr_scheduler.LinearLR(FullPINNOptimizer, start_factor=1.0, end_factor=0.1, total_iters=1)\n",
    "\n",
    "    scalingfactor = torch.tensor([[temporalNormalization,spacialNormalization]],requires_grad=False).to(device)\n",
    "\n",
    "    dataIn = torch.tensor(data[0][:,0:2].real,dtype=torch.float32).to(device)\n",
    "    dataIn = dataIn * scalingfactor\n",
    "    expectedRe = []\n",
    "    expectedIm = []\n",
    "    for i in range(numoutputs):\n",
    "        expectedRe.append(torch.tensor(data[i][:,2].real,dtype=torch.float32).to(device))\n",
    "        expectedIm.append(torch.tensor(data[i][:,2].imag,dtype=torch.float32).to(device))\n",
    "   \n",
    "    data = colocPoints(colocationPoints[~tEqualsZeroMask,:])\n",
    "\n",
    "    #initializing the dataloader\n",
    "    dataloader = DataLoader(data, batch_size=5000, shuffle=False)\n",
    "\n",
    "    bestLoss = 9999999\n",
    "    bestOutmodel = copy.deepcopy(outmodel)\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        FullPINNOptimizer.zero_grad()\n",
    "\n",
    "        if epoch <= ODEWeightEpoch:\n",
    "            FullPINNOptimizer.zero_grad()\n",
    "            dataOut = outmodel(Reservoir(dataIn))\n",
    "            DataLoss = 0\n",
    "            for i in range(numoutputs):\n",
    "                DataLoss += loss_fn(dataOut[:, 2*i], expectedRe[i])\n",
    "                DataLoss += loss_fn(dataOut[:, 2*i + 1], expectedIm[i])\n",
    "\n",
    "            #getting the average data loss\n",
    "            DataLoss /= 2*numoutputs\n",
    "\n",
    "            ICout = outmodel(Reservoir(colocationPoints[tEqualsZeroMask,:]))\n",
    "            ICLoss = loss_fn(ICout,ICs)\n",
    "\n",
    "            loss = ICWeight*ICLoss + DataWeight*DataLoss\n",
    "            loss.backward()\n",
    "            FullPINNOptimizer.step()\n",
    "            \n",
    "            ODEloss = 1\n",
    "            BCLoss = 1\n",
    "\n",
    "        else:\n",
    "            for i, batchdata in enumerate(dataloader):\n",
    "                FullPINNOptimizer.zero_grad()\n",
    "\n",
    "                colocs = batchdata[\"X\"].to(device).detach().clone().requires_grad_(True)\n",
    "\n",
    "                dataOut = outmodel(Reservoir(dataIn))\n",
    "                DataLoss = 0\n",
    "                for i in range(numoutputs):\n",
    "                    DataLoss += loss_fn(dataOut[:,2*i],expectedRe[i])\n",
    "                    DataLoss += loss_fn(dataOut[:,2*i + 1],expectedIm[i])\n",
    "                #getting the average data loss\n",
    "                DataLoss /= 2*numoutputs\n",
    "\n",
    "                ICout = outmodel(Reservoir(colocationPoints[tEqualsZeroMask,:]))\n",
    "                ICLoss = loss_fn(ICout,ICs)\n",
    "\n",
    "                #left boundry mask: When applied to the output, it will return the outputs on the left boundry\n",
    "                LBMask = torch.isclose(colocs[:, 1], torch.tensor(LeftBoundary, dtype=colocs.dtype)).to(device)\n",
    "                #right boundry mask: When applied to the output, it will return the outputs on the right boundry\n",
    "                RBMask = torch.isclose(colocs[:, 1], torch.tensor(rightBoundary, dtype=colocs.dtype)).to(device)\n",
    "\n",
    "                '''                note: In order for the outputs after the application of the masks to corrispond to the same times,\n",
    "                colocationPoints must be orginised as follows:\n",
    "\n",
    "                colocationPoints = [[t1,x1],[t1,x2],[t1,x3],[t1,x4],[t1,x5],[t1,x6],...,[t1,xn],\n",
    "                                    [t2,x1],[t2,x2],[t2,x3],[t2,x4],[t2,x5],[t2,x6],...,[t2,xn], ...]\n",
    "\n",
    "                This yields:\n",
    "                colocationPoints[LBMask] = [ [t1,x1], [t2,x1], [t3,x1], ...]\n",
    "                colocationPoints[RBMask] = [ [t1,xn], [t2,xn], [t3,xn], ...]\n",
    "                '''\n",
    "                evenmask = torch.tensor([(i % 2 == 0) for i in range(2*numoutputs)]).to(device)\n",
    "                oddmask = torch.tensor([(i % 2 != 0) for i in range(2*numoutputs)]).to(device)\n",
    "\n",
    "                #scaling network input\n",
    "                networkInput = colocs*scalingfactor\n",
    "\n",
    "                ResOutOverEvaluationPoints = Reservoir(networkInput)\n",
    "                output = outmodel(ResOutOverEvaluationPoints) # shape: [N, 2D]\n",
    "\n",
    "                firstTDerivatives = []\n",
    "                firstXDerivatives = []\n",
    "                secondXDerivatives = []\n",
    "               \n",
    "                # colocationPoints[:,0] = t, colocationPoints[:,1] = x\n",
    "                for i in range(output.shape[1]):\n",
    "                    out = output[:, i]  # shape: [N]\n",
    "                    grad1 = torch.autograd.grad(\n",
    "                        out, colocs,\n",
    "                        grad_outputs=torch.ones_like(out),\n",
    "                        create_graph=True, retain_graph=True\n",
    "                    )[0]  # [N, input_dim]\n",
    "                    grad2 = torch.autograd.grad(\n",
    "                        grad1[:,1], colocs,\n",
    "                        grad_outputs=torch.ones_like(grad1[:,1]),\n",
    "                        create_graph=True, retain_graph=True\n",
    "                    )[0]  # [N, input_dim]\n",
    "\n",
    "                    #this is equivilant to taking grad1[:,0].reshape(-1,1) withough having to call reshape\n",
    "                    firstTDerivatives.append(grad1[:,0:1])\n",
    "                    firstXDerivatives.append(grad1[:,1:2])\n",
    "                    secondXDerivatives.append(grad2[:,1:2])\n",
    "\n",
    "                OutputFirstTDerivative = torch.cat(firstTDerivatives, dim=1).to(device) # shape: [N, 2D]\n",
    "                OutputFirstXDerivative = torch.cat(firstXDerivatives, dim=1).to(device) # [N, 2D]\n",
    "                OutputSecondXDerivative = torch.cat(secondXDerivatives, dim=1).to(device) # [N, 2D]\n",
    "\n",
    "                #using evaluation points as colocation points\n",
    "                ODEloss = SchrodingerEqnResidualLoss(output, OutputFirstTDerivative,OutputSecondXDerivative,colocs[:,1], outmodel,evenmask,oddmask)\n",
    "\n",
    "                #enforcing the boundary conditions for t!=0\n",
    "                zero = torch.zeros(output[LBMask,:].shape).to(device)\n",
    "                BCLoss = loss_fn(output[LBMask,:],zero) + loss_fn(OutputFirstXDerivative[LBMask,:],zero)\n",
    "                BCLoss += loss_fn(output[RBMask,:],zero) + loss_fn(OutputFirstXDerivative[RBMask,:],zero)\n",
    "\n",
    "                loss = ODEWeight*ODEloss + BCWeight*BCLoss + ICWeight*ICLoss + DataWeight*DataLoss\n",
    "\n",
    "                #using backpropogation to get the derivitive of the parameters in the network with respect to the loss\n",
    "                loss.backward()\n",
    "                #taking a step in the direction of negative slope according to our optimiser\n",
    "                FullPINNOptimizer.step()\n",
    "\n",
    "                if (loss.item() < bestLoss):\n",
    "                    bestLoss = loss.item()\n",
    "                    bestOutmodel = copy.deepcopy(outmodel)\n",
    "            if (epoch-ODEWeightEpoch-1) == 1000:\n",
    "                FullPINNOptimizer = optim.LBFGS(list(Reservoir.parameters()) + list(outmodel.parameters()), lr=lr)\n",
    "            #loss weight scheduling\n",
    "            if (epoch-ODEWeightEpoch-1) <= 2000:\n",
    "                ODEWeight = initialODEWeight + ((epoch-ODEWeightEpoch-1)/2000)*100*initialODEWeight\n",
    "                #if (epoch-ODEWeightEpoch-1)%500 == 0:\n",
    "                #    DataWeight = DataWeight*0.5\n",
    "            if (epoch-ODEWeightEpoch-1) == 2000:\n",
    "                scheduler.step()\n",
    "\n",
    "        #loss = ICWeight*ICLoss + BCWeight*BCLoss + ODEWeight*ODEloss# + DataWeight*DataLoss\n",
    "        epsilon = 1e-12\n",
    "        wandb.log({\"Log Loss\": math.log(loss.item()+epsilon),\"Log PDE Loss\": math.log(ODEloss+epsilon),\"Log IC Loss\": math.log(ICLoss+epsilon),\"Log BC Loss\": math.log(BCLoss+epsilon),\"Log Data Loss\": math.log(DataLoss+epsilon)})\n",
    "\n",
    "        averageLossOverTime.append(loss.item())\n",
    "\n",
    "        if verbose:\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"-------------------------------\\nEpoch {epoch}\")\n",
    "                print(f\"Average Loss: {loss.item()}\")\n",
    "                print(f\"ODEL = {ODEloss}\")\n",
    "                print(f\"ICL = {ICLoss}\")\n",
    "                print(f\"BCL = {BCLoss}\")\n",
    "\n",
    "    print(f\"Average Loss: {loss.item()}\")\n",
    "\n",
    "    return (Reservoir,bestOutmodel,averageLossOverTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "115619f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name=\"Block\"):\n",
    "    start = perf_counter()\n",
    "    yield\n",
    "    end = perf_counter()\n",
    "    print(f\"[{name}] Elapsed time: {end - start:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afea86e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline training Start!\n",
      "training file used:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_10000DPts/dataKis5.0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m data = []\n\u001b[32m     19\u001b[39m trueParameters = [[]]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m data_i = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata_10000DPts/dataKis\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[32;43m15\u001b[39;49m\u001b[43m/\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m trueParameters[\u001b[32m0\u001b[39m].append(\u001b[32m15\u001b[39m/\u001b[32m3\u001b[39m)\n\u001b[32m     22\u001b[39m data.append(data_i)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PINNs/.PINNs/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    425\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    428\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data_10000DPts/dataKis5.0.npy'"
     ]
    }
   ],
   "source": [
    "print(\"Offline training Start!\")\n",
    "with timer(\"Training Loop\"):\n",
    "    torch.manual_seed(42)\n",
    "    #assigning device as done in tutorial\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    '''\n",
    "    data = []\n",
    "    maxK = 5\n",
    "    numpointsperunit = 3\n",
    "    print(\"training files used:\")\n",
    "    for i in range(0,maxK*numpointsperunit + 1,numpointsperunit):\n",
    "        data_i = np.load(f\"data_10000DPts/dataKis{i/numpointsperunit}.npy\", allow_pickle=True)\n",
    "        data.append(data_i)\n",
    "        print(f\"data/dataKis{i/numpointsperunit}.npy\")\n",
    "    '''\n",
    "    print(\"training file used:\")\n",
    "    data = []\n",
    "    trueParameters = [[]]\n",
    "    data_i = np.load(f\"data_10000DPts/dataKis{15/3}.npy\", allow_pickle=True)\n",
    "    trueParameters[0].append(15/3)\n",
    "    data.append(data_i)\n",
    "    print(f\"data/dataKis{15/3}.npy\")\n",
    "    trueParameters = torch.tensor(trueParameters).to(device)\n",
    "\n",
    "    LeftBoundary = -5\n",
    "    RightBoundary = 5\n",
    "    domainDiameter = RightBoundary - LeftBoundary\n",
    "    domainCenter = (LeftBoundary + RightBoundary)/2\n",
    "    timeRange = 1.5\n",
    "    numxvals = 100\n",
    "    numtvals = 100\n",
    "\n",
    "    # Generate evenly spaced points in time and space\n",
    "    tvals = np.linspace(0, timeRange, numtvals)\n",
    "    xvals = np.linspace(LeftBoundary, RightBoundary, numxvals)\n",
    "\n",
    "    colocationPoints = [[t, x] for t in tvals for x in xvals]\n",
    "\n",
    "    colocationPoints = torch.tensor(colocationPoints,dtype=torch.float32,requires_grad=True).to(device)\n",
    "\n",
    "    resWidth = 100\n",
    "    resDepth = 5\n",
    "\n",
    "    nummodels = len(data)\n",
    "\n",
    "    domainDiameter = (RightBoundary - LeftBoundary)/2\n",
    "\n",
    "    kx    = 0.1                        # wave number\n",
    "    m     = 1                          # mass\n",
    "    sigma = 0.5                   # width of initial gaussian wave-packet\n",
    "\n",
    "    A = 1.0 / (sigma * math.sqrt(torch.pi)) # normalization constant\n",
    "\n",
    "    ICs = torch.zeros((numxvals,2*nummodels),dtype=torch.float32).to(device)\n",
    "\n",
    "    #setting initial condiitons to u(0,x) = 2sech(x + offset) as done in [Raissi et al. 2019]\n",
    "    #xvals = torch.tensor(xvals).reshape(-1).to(device)\n",
    "    xvals = xvals.reshape(-1)\n",
    "    #offset = ((torch.rand((nummodels))-0.5)*domainDiameter + domainCenter).to(device)\n",
    "    offset = np.ones((nummodels))*-2\n",
    "    for i in range(nummodels):\n",
    "        #ICs[:,2*i] = 2*(torch.cosh(xvals + offset[i]).pow(-1))\n",
    "        IC = math.sqrt(A) * np.exp(-(xvals-offset[i])**2 / (2.0 * sigma**2)) * np.exp(1j * kx * xvals)\n",
    "        ICs[:,2*i] = torch.tensor(IC.real).to(device)\n",
    "        ICs[:,2*i + 1] = torch.tensor(IC.imag).to(device)\n",
    "\n",
    "    #input and hidden layers\n",
    "    Reservoir = MLPWithoutOutput(2,resWidth,resDepth).to(device)\n",
    "\n",
    "    #initilizing loss function\n",
    "    loss_fn = nn.MSELoss().to(device)\n",
    "\n",
    "    #setting the number of training epochs\n",
    "    trainingEpochs = 3200\n",
    "    trainlr = 2e-3\n",
    "\n",
    "    averageLossOverTime = []\n",
    "\n",
    "    ODEWeight = 5e-4\n",
    "    ICWeight = 1#10\n",
    "    BCWeight = 1e-3#1\n",
    "    DataWeight = 1\n",
    "\n",
    "    wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Time Dependent Schrodinger Equation (Standard PINN)\",\n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": trainlr,\n",
    "      \"epochs\": trainingEpochs,\n",
    "      \"ODE_weight\": ODEWeight,\n",
    "      \"resWidth\": resWidth,\n",
    "      \"num_models\": nummodels,\n",
    "      \"resDepth\": resDepth\n",
    "      })\n",
    "    tscale = 1\n",
    "    xscale = 1\n",
    "\n",
    "    outmodel = MLPOutput(resWidth,2*nummodels).to(device)\n",
    "    #Reservoir, averageLossOverTime = trainFullNetworkWithPrecomputing(Reservoir,data,(1/timeRange),(1/RightBoundary),outmodel,nummodels,ICs,LeftBoundary,RightBoundary,colocationPoints,ODEWeight,ICWeight,BCWeight,DataWeight,trainingEpochs,loss_fn,trainlr,averageLossOverTime,device,verbose=False)\n",
    "    Reservoir,outmodel,averageLossOverTime = trainFullNetworkWithPrecomputing(Reservoir,data,(tscale),(xscale),outmodel,nummodels,ICs,LeftBoundary,RightBoundary,colocationPoints,ODEWeight,ICWeight,BCWeight,DataWeight,trainingEpochs,loss_fn,trainlr,averageLossOverTime,device,verbose=False)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outmodel.parameterSet)\n",
    "parameterLoss = loss_fn(outmodel.parameterSet,trueParameters)\n",
    "meanAbsoluteParameterError = torch.mean(torch.abs(trueParameters - outmodel.parameterSet))\n",
    "print(f\"Parameter MSE = {parameterLoss.item()}\")\n",
    "print(f\"Parameter MAE = {meanAbsoluteParameterError}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be71a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss = torch.log(torch.tensor(averageLossOverTime))\n",
    "plt.plot(logloss.detach().numpy())\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training Log Average Loss Of The 30 Readout Layers')\n",
    "plt.grid(True)\n",
    "plt.savefig('Training Log Average Loss Of The 30 Readout Layers.png')\n",
    "#plt.show()\n",
    "\n",
    "plt.close()\n",
    "\n",
    "# Plotting code below generated by chatGPT\n",
    "\n",
    "# Ensure tvals is a tensor\n",
    "tvals = torch.tensor(tvals, dtype=torch.float32)\n",
    "\n",
    "# Recreate the grid of test points\n",
    "test_points = torch.tensor([[t.item(), x.item()] for t in tvals for x in xvals],\n",
    "                           dtype=torch.float32).to(device)\n",
    "\n",
    "scalingfactor = torch.tensor([[(tscale),(xscale)]],requires_grad=False).to(device)\n",
    "\n",
    "# Evaluate the network\n",
    "with torch.no_grad():\n",
    "    res_output = Reservoir(test_points*scalingfactor)\n",
    "    prediction = outmodel(res_output)  # shape: [T*X, 2*nummodels]\n",
    "\n",
    "# Convert to numpy and reshape\n",
    "prediction = prediction.cpu().numpy()  # shape: [T*X, 2*nummodels]\n",
    "\n",
    "num_rows = int(np.ceil(nummodels))\n",
    "fig, axes = plt.subplots(num_rows, 3, figsize=(12, 4 * num_rows))\n",
    "\n",
    "if num_rows == 1:\n",
    "    axes = [axes]  # make iterable\n",
    "\n",
    "\n",
    "for i in range(nummodels):\n",
    "    real = prediction[:, 2 * i].reshape(len(tvals), len(xvals))\n",
    "    imag = prediction[:, 2 * i + 1].reshape(len(tvals), len(xvals))\n",
    "    magnitude = np.sqrt(real**2 + imag**2)\n",
    "\n",
    "    ax_real, ax_imag, ax_mag = axes[i]\n",
    "\n",
    "    im0 = ax_real.imshow(real, extent=[xvals[0], xvals[-1], tvals[-1], tvals[0]],\n",
    "                         aspect='auto', cmap='RdBu_r')\n",
    "    ax_real.set_title(f\"Model {i+1}: Re(u)\")\n",
    "    fig.colorbar(im0, ax=ax_real)\n",
    "\n",
    "    im1 = ax_imag.imshow(imag, extent=[xvals[0], xvals[-1], tvals[-1], tvals[0]],\n",
    "                         aspect='auto', cmap='RdBu_r')\n",
    "    ax_imag.set_title(f\"Model {i+1}: Im(u)\")\n",
    "    fig.colorbar(im1, ax=ax_imag)\n",
    "\n",
    "    im2 = ax_mag.imshow(magnitude, extent=[xvals[0], xvals[-1], tvals[-1], tvals[0]],\n",
    "                        aspect='auto', cmap='viridis')\n",
    "    ax_mag.set_title(f\"Model {i+1}: |u|\")\n",
    "    fig.colorbar(im2, ax=ax_mag)\n",
    "\n",
    "    for ax in (ax_real, ax_imag, ax_mag):\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"t\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Predicted psi(t,x).png')\n",
    "\n",
    "plt.close()\n",
    "\n",
    "plt.plot(xvals, prediction[:len(xvals), 2 * i], label=\"Re(u) at t=0\")\n",
    "plt.plot(xvals, ICs[:, 2 * i].cpu().numpy(), label=\"IC\")\n",
    "plt.legend()\n",
    "plt.title(\"Initial Condition vs Model Output at t=0\")\n",
    "plt.grid(True)\n",
    "plt.savefig('insanity check.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".PINNs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
